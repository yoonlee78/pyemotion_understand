{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Chapter 3. Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "앞 장에 이어 이번 장의 주제도 단어의 분산 표현이다. 앞 장에서는 '통계 기반 기법'으로 단어의 분산 표현을 얻었지만, 이번 장에서는 더 강력한 '추론 기반 기법'에 대해 알아보겠다. \n",
    "\n",
    "'추론 기반 기법'은 추론을 하는 기법이다. 이 추론 과정에서 신경망을 이용한다. 그 중 유명한 word2vec에 대해 알아보겠다. \n",
    "\n",
    "이번 장의 목표는 '단순한' word2vec 구현하기이다. 처리 효율을 희생하는 대신 이해하기 쉽도록 아주 단순한 word2vec의 구조에 대해 알아보겠다. 따라서 큰 데이터셋 대신, 작은 데이터셋으로 먼저 처리할 수 있도록 구성할 것이다. 다음 장에서 이번 장에서 만든 단순한 word2vec을 개선하여 '진짜' word2vec을 완성시킬 예정이다. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3.1. 추론 기반 기법과 신경망\n",
    "\n",
    "## 3.1.1. 통계 기반 기법의 문제점\n",
    "\n",
    "통계 기반 기법에서 흔히 하듯, 단어를 표현할때 주변 단어의 빈도를 기초로 했다. 단어의 동시발생 행렬을 만들고 그 행렬에 SVD를 적용하여 밀집 벡터 (단어의 분산 표현)을 얻어냈지만, 대규모 말뭉치를 다룰 때 문제가 발생한다. \n",
    "\n",
    "현업에서 다루는 말뭉치의 어휘 수는 어마어마하다. 예컨데 영어의 어휘 수는 100만개를 훌쩍 넘는다. 어휘가 100개라면, 통계 기반 기법에서는 '100만 * 100만'이라는 거대한 행렬을 만들어야 한다. 이러한 거대한 행렬에 SVD를 적용하는 일은 현실적이지 않다. \n",
    "\n",
    "또한, 통계 기반 기법은 말뭉치 전체의 통계 (동시발생 행렬과 PPMI등)를 이용해 단 1회의 처리 (SVD 등)에 단어의 분산 표현을 얻는다. 한편, 추론 기반 기법에서는, 예컨데 신경망을 이용하는 경우는 미니배치로 학습하는 것이 일반적이다. \n",
    "\n",
    "미니배치 학습에서는 신경망이 한번에 소량 (미니배치)의 학습 샘플씩 반복해서 학습하며 가중치를 생신해간다 \n",
    "\n",
    "[그림 3-1]두 기법의 큰 차이 (통계 기반 기법과 추론 기반 기법 비교)\n",
    "\n",
    "<img src = \"./images_equations/fig3-1.png\" width=400>\n",
    "\n",
    "## 3.1.2. 추론 기반 기법 개요\n",
    "\n",
    "추론 기반 기법의 주된 작업은 '추론'이다. \n",
    "추론이란 아래 그림3-2처럼 주변 단어(맥락)가 주어졌을 때 '?'에 무슨 단어가 들어가는지를 추측하는 작업이다. \n",
    "\n",
    "<img src = \"./images_equations/fig 3-2.png\" width=400>\n",
    "\n",
    "위 그림처럼 **추론 문제를 풀고 학습하는 것이 '추론 기반 기법'**이다. 이러한 추론 문제를 반복해서 풀면서 단어의 출현 패턴을 학습한다. '모델 관점'에서 보면, 이 추론 문제는 [그림3-3]처럼 보인다. \n",
    "\n",
    "<img src = \"./images_equations/fig 3-3.png\" width=400>\n",
    "\n",
    "위 그림3-3처럼 추론 기반 기법에는 어떠한 모델이 등장한다. 우리는 이 모델로 신경망을 사용한다. \n",
    "여기서 모델은 '맥락'정보를 입력바다 (출현 가능한) 각 단어의 출현 **확률**을 출력한다. \n",
    "이러한 틀 안에서 말뭉치를 사용해 모델이 올바른 추측을 내놓도록 학습시킨다. \n",
    "그리고 그 학습의 결과로 단어의 분산 표현을 얻는 것이 추론 기반 기법의 전체 그림이라고 할 수 있다. \n",
    "\n",
    "(추론 기반 기법도 통계 기반 기법처럼 분포 가설에 기초한다. \n",
    "분포 가설이란, 단어의 의미는 주변 단어에 의해 형성된다-는 가설로, 추론 기반 기법에서는 이를 앞에서와 같은 \n",
    "추측 문제로 귀결시킨다. 이처럼 두 기법 모두 분포 가설에 근거하는 '단어의 동시발생 가능성'을 얼마나 잘 모델링하는가가 중요한 연구주제이다)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.1.3. 신경망에서의 단어 처리\n",
    "\n",
    "지금부터 신경망을 이용해 '단어'를 처리해보자. \n",
    "신경망은 'you', 'say' 등의 **글자 그대로의** 단어를 처리할 수 없다 (즉 계산할 수 없다). \n",
    "따라서 단어를 '고정 길이의 벡터'로 변환해야한다. \n",
    "이때 사용하는 대표적인 방법이 '원핫 표현 (또는 원핫 벡터)'로 변환하는 것이다. \n",
    "원핫 표현이란 벡터의 원소 중 하나만 1이고 나머지는 모두 0인 벡터를 말한다. \n",
    "\n",
    "원핫 표현에 대해 구체적으로 살펴보자.\n",
    "\n",
    "<img src = \"./images_equations/fig 3-4.png\" width=400>\n",
    "\n",
    "\n",
    "<img src = \"./images_equations/fig 3-5.png\" width=400>\n",
    "\n",
    "\n",
    "<img src = \"./images_equations/fig 3-6.png\" width=400>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src = \"./images_equations/fig 3-7.png\" width=400>\n",
    "\n",
    " 지금까지의 완전연결계층에 의한 변환은 파이썬으로 다음과 같다:\n",
    "\n",
    "아래 코드는 ID가 0인 단어를 원한 표현으로 표현한 다음, 완전연결계층을 통과시켜 변환한다. \n",
    "완전연결계층(FCN)의 계산은 행렬 곱으로 수행할 수 있고, 행렬 곱은 넘파이의 np.matmul()이 해결해준다 (편향은 생략한다)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.59169804  1.13436143  0.79875587]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "c = np.array([1,0,0,0,0,0,0]) #입력\n",
    "W = np.random.randn(7,3) #가중치\n",
    "h = np.matmul(c,W) #중간 노드\n",
    "\n",
    "print(h)\n"
   ]
  },
  {
   "source": [
    "앞의 코드에서 주목할 곳은 c와 W의 행렬 곱 부분이다. \n",
    "c 는 원핫 표현이므로 단어ID에 대응하는 원소만 1이고 그 외에는 0인 벡터다. 따라서 앞 코드의 c와 W의 행렬 곱은 결국 [그림3-8]처럼 가중치의 행벡터 하나를 \"뽑아낸\" 것과 같다. \n",
    "\n",
    "<img src = \"./images_equations/fig 3-8.png\" width=400>\n",
    "\n",
    "그저 가중치로부터 행벡터를 뽑아낼 뿐인데 행렬 곱을 계산하는 건 비효율적이라고 생각될 수 있다. \n",
    "이 점은 '4.1word2vec 개선(1)' 절에서 개선할 예정이다. \n",
    "또한, 앞의 코드로 수행한 작업은 (1장에서 구현한) MatMul 계층으로도 수행할 수 있다. (아래 코드 참고)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/Users/yklee/study/deep-learning-from-scratch-2'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-0.34702562 -1.90623624  0.24143819]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common import layers \n",
    "\n",
    "c = np.array([1,0,0,0,0,0,0]) #입력\n",
    "W = np.random.randn(7,3)\n",
    "layer = MatMul(W)\n",
    "h=layer.forward(c)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# 3.2. 단순한 word2vec\n",
    "\n",
    "앞 절에서는 추론 기반 기법을 배우고, 신경망으로 단어를 처리하는 방법을 코드로 살펴봄.\n",
    "이제 word2vec(아주 간단한 것)을 구현해보자\n",
    "\n",
    "지금부터 할 일은 [그림3-3]의 '모델'을 신경망으로 구축하는 것이다. \n",
    "이번 절에서 사용할 신경망은 word2vec에서 제안하는 CBOW(continuous bag-of-words)모델이다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.2.1. CBOW 모델의 추론 처리\n",
    "\n",
    "- CBOW 모델은 맥락으로부터 타깃(target)을 추측하는 용도의 신경망이다. ('타깃'은 중앙 단어이고 그 주변 단어들이 '맥락'이다) \n",
    "- 우리는 이 CBOW 모델이 가능한 한 정확하게 추론하도록 훈련시켜서 단어의 분산 표현을 얻어낼 것이다.\n",
    "\n",
    "- CBOW 모델의 입력은 '맥락'이다. 맥락은 'you', 'goodbye'같은 단어들의 **목록**이다. \n",
    "가장 먼저, 이 맥락을 원핫 표현으로 변환하여 CBOW모델이 처리할 수 있도록 준비한다. \n",
    "이상을 기초로 CBOW모델의 신경망을 아래 [그림3-9]처럼 그릴 수 있다. \n",
    "\n",
    "<img src = \"./images_equations/fig 3-9.png\" width=400>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- 위 [그림3-9]가 CBOW모델의 신경밍이다. \n",
    "- 입력층이 2개이고 은닉층을 거쳐 출력층에 도달한다. \n",
    "- 두 입력층에서 은닉층으로의 변환은 똑같은 완전연결계층(가중치는 W_in)이 처리한다. \n",
    "- 그리고 은닉층에서 출력층 뉴런으로의 변환은 다른 완전연결계층(가중치는 W_out)이 처리한다. \n",
    "\n",
    "<은닉층>\n",
    "\n",
    "- [그림3-9]의 은닉층에 주목하자. 은닉층의 뉴런은 입력층의 완전연결계층에 의해 변환된 값이 되는데, 입력층이 여러 개이면 전체를 '평균'하면 된다. 앞의 예에 대입해보면 다음과 같다. \n",
    "- 완전연결계층에 의한 첫 번째 입력층이 h_1으로 변환되고, 두번째 입력층이 h_2로 변환되었다고 하면, 은닉층 뉴런은 1/2(h_1+h_2)가 된다. \n",
    "\n",
    "<출력층>\n",
    "\n",
    "- 총 7개의 뉴런 (여기서 중요한 점은 이 뉴런 하나하나가 각각의 단어에 대응한다는 점이다) \n",
    "- 출력층 뉴런은 각 단어의 '점수'를 뜻한다. 값이 높을수록 대응 단어의 출현 확률도 높아진다. \n",
    "- 여기에 나오는 '점수'는 확률로 해석되기 **전**의 값이다. 이 점수에 **소프트맥스**(softmax)함수를 적용해서 '확률'을 얻을 수 있다\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src = \"./images_equations/fig 3-10.png\" width=400>\n",
    "\n",
    "- 가중치의 각 행이 해당 단어의 분산 표현이다.\n",
    "\n",
    "\n",
    "\n",
    "- [그림 3-10]에 나오는 가중치(W_in)의 각 행에는 해당 단어의 분산 표현이 담겨있다. \n",
    "- 학습을 진행할수록 맥락에서 (단어 목록) 출현하는 단어를 잘 추측할 수 있도록 이 분산 표현이 갱신될 것이다 (업데이트)\n",
    "- 은닉층의 뉴런수를 입력층의 뉴런 수보다 적게 하는 것이 중요하다. 이렇게 해야 은닉층에는 단어 예측에 필요한 인코딩 정보를 '간결하게' 담게 되며, 결과적으로 밀집벡터 표현을 얻을 수 있다. 인풋->은닉층 = 인코딩. 은닉층 => 결과: 디코딩. 즉, 디코딩이란 인코딩된 정보를 우리 인간이 이해할 수 있는 표현으로 복원하는 작업이다.\n",
    "- 이렇게해서 얻은 벡터에는 '단어의 의미'도 잘 녹아들어있다. 이것이 word2vec의 전체 그림이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<img src = \"./images_equations/fig 3-11.png\" width=400>\n",
    "\n",
    "- 계층 관점에서 본 CBOW모델의 신경망 구성: MatMul 계층에서 사용하는 가중치(W_in, W_out)는 해당 계층 안으로 넣었다. \n",
    "\n",
    "지금까지 우리는 CBOW모델을 '뉴런 관점'에서 그렸다. 이번에는 '계층 관점'에서 그려보자. \n",
    "\n",
    "위 그림 3-11은 그림 3-9의 신경망을 계층 관점에서 그린 모습이다. \n",
    "\n",
    "그림에서 알 수 있듯이 CBOW 모델의 가장 앞 단에는 2개의 MatMul 계층이 있고, 이어서 이 두 계층의 출력이 더해진다. 그리고 더해진 값에 0.5를 곱하면 '평균'이 된다. 이 평균이 은닉층의 뉴런이 된다. \n",
    "\n",
    "마지막으로 은닉층 뉴런에 또 다른 MatMul계층이 적용되어 '점수score'가 출력된다. \n",
    "(편향을 사용하지 않는 완전연결계층의 처리는 MatMul layer의 순전파와 같다. 이 계층은 내부에서 행렬 곱을 계산한다)\n",
    "\n",
    "\n",
    "그림 3-11을 참고하여 cbow모델의 추론 처리를 파이썬으로 구현해보자. \n",
    "\n",
    "_추론 처리_란, '점수'를 구하는 처리를 말한다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.15404488 0.69272704 0.36943414 0.10675218 0.5155834  1.32430336\n  0.71115208]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul\n",
    "\n",
    "\n",
    "# 샘플 맥락 데이터\n",
    "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
    "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "# 가중치 초기화 \n",
    "W_in = np.random.randn(7, 3)\n",
    "W_out = np.random.randn(3, 7)\n",
    "\n",
    "# 계층 생성 (입렻긍을 처리하는 matmul계층을 맥락 수만큼 (2개) 생성하고, 출력층 측의 matmul은 1개만 생성) 여기서 matmul계층은 가중치 w_in을 공유한다는 점에 주의하자.\n",
    "in_layer0 = MatMul(W_in)\n",
    "in_layer1 = MatMul(W_in)\n",
    "out_layer = MatMul(W_out)\n",
    "\n",
    "# 순전파\n",
    "h0 = in_layer0.forward(c0)\n",
    "h1 = in_layer1.forward(c1)\n",
    "h = 0.5 * (h0 + h1)\n",
    "s = out_layer.forward(h)\n",
    "print(s) "
   ]
  },
  {
   "source": [
    "## 3.2.2. CBOW모델의 학습\n",
    "\n",
    "지금까지 설명한 CBOW 모델은 출력층에서 각 단어의 점수를 출력함. 이 점수에 소프트맥수 함수를 적용하면 '확률'을 얻을 수 있음. (그림 3-12)\n",
    "이 확률은 맥락 (전후 단어)이 주어졌었을 때 그 중앙에 어떤 단어가 출현하는지를 나타낸다. \n",
    "\n",
    "[그림3-12]의 예에서 맥락은 'you'와 'goodbye'이고, 정답 레이블(신경망이 예측해야할 단어)은 'say'이다. \n",
    "이때 '가중치가 적절히 설정된' 신경망이라면 '확률'을 나타내는 뉴런들 중 정답에 해당하는 뉴런의 값이 클 것이라 기대할 수 있다. \n",
    "\n",
    "\n",
    "모델 학습에서는 올바른 예측을 할 수 있도록 가중치를 조정하는 일을 한다. 그 결과로 가중치 W_in에 (정확히는 W_in과 W_out모두에) 단어의 출현 패턴을 파악한 벡터가 학습된다\n",
    "\n",
    "<img src = \"./images_equations/fig 3-12.png\" width=400>\n",
    "\n",
    "CBOW모델의 구체적인 예 (노드 값의 크기를 흑백의 진하기로 나타냄)\n",
    "\n",
    "<img src = \"./images_equations/fig 3-13.png\" width=400>\n",
    "\n",
    "CBOW모델의 학습 시 신경망 구성\n",
    "\n",
    "<img src = \"./images_equations/fig 3-14.png\" width=400>\n",
    "\n",
    "Softmax계층과 Cross Entropy Error 계층을 Softmax with Loss 계층으로 합침 \n",
    "\n",
    "### 3.2.3. word2vec의 가중치와 분산 표현\n",
    "\n",
    "- word2vec에 사용되는 신경망에는 두 가지 종류가 있다. \n",
    "- 입력측, 출력측, 양쪽 가중치 중 어떤 것을 사용하느냐에 따라 다르다\n",
    "- word2vec(특히 skip-gram)에서는 '입력 측의 가중치만 이용한다'가 가장 대중적인 선택이다. 많은 연구에서 출력 측 가중치는 버리고 입력 측 가중치 W_in만을 최종 단어의 분산 표현으로서 이용한다. 우리도 이를 따라서  W_in을 단어의 분산 표현으로 이용할 것이다. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3.3. 학습 데이터 준비\n",
    "\n",
    "\"You say goodbye and I say hello\"라는 한 문장짜리 코퍼스를 예시로 써보자.\n",
    "\n",
    "## 3.3.1. 맥락과 타깃\n",
    "\n",
    "<img src = \"./images_equations/fig 3-16.png\" width=400>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1 2 3 4 1 5]\n{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello'}\n"
     ]
    }
   ],
   "source": [
    "#말뭉치로부터 목표로 하는 단어를 '타깃'으로, 그 주변 단어를 '맥락'으로 뽑아내자.\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.util import preprocess\n",
    "\n",
    "text = \"You say goodbye and I say hello\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "print(corpus)\n",
    "\n",
    "print(id_to_word)"
   ]
  },
  {
   "source": [
    "\n",
    "<img src = \"./images_equations/fig 3-17.png\" width=400>\n",
    "\n",
    "- 단어의 ID 의 배열인 corpus로부터 맥락과 타깃을 작성하는 예 (맥락의 윈도우 크기는 1)\n",
    "- 맥락은 2차원 배열이다. 이때 맥락의 0번째 차원에는 각 맥락 데이터가 저장된다. 정확히 말하면 contexts[0]에는 0번째 맥락이 저장되고, contexts[1]에는 1번째 맥락이 저장되는 식이다. 마찬가지로 타깃에서도 target[0]에는 0번째 타깃이, target[1]에는 1번째 타깃이 저장된다. \n",
    "\n",
    "- 다음은 이 맥락과 타깃을 만드는 함수를 구현할 차례이다. \n",
    "create_contexts_target(corpus,window_size)라는 이름으로 다음과 같이 구현하자 (common/util.py)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''맥락과 타깃 생성\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "source": [
    "- 이 함수는 인수를 두 개 받는다. \n",
    "- 하나는 단어ID의 배열(corpus), 다른 하나는 맥락의 윈도우 크기 (window_size)\n",
    "- 그리고 맥락과 타겟을 각각 넘파이 다차원 배열로 돌려준다. 이 함수를 실제로 사용해보겠다. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 2]\n [1 3]\n [2 4]\n [3 1]\n [4 5]]\n[1 2 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "print(contexts)\n",
    "\n",
    "print(target)"
   ]
  },
  {
   "source": [
    "이렇게 말뭉치(corpus)로부터 맥락과 타겟을 만들어보았다. \n",
    "나중에 이를 CBOW모델에 넘겨주면 된다. 그러나 아직 이 맥락과 타겟의 각 원소가 여전히 단어ID이다. 그럼 이어서 이를 원핫 표현으로 변환해보겠다"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.3.2. 원핫 표현으로 변환\n",
    "계속해서 맥락과 타겟을 원핫 표현으로 바꿔보자. 이때 수행하는 변환은 [그림3-18]과 같다. \n",
    "<img src = \"./images_equations/fig 3-18.png\" width=400>\n",
    "\n",
    "\n",
    "[그림3-18]의 예처럼 맥락과 타겟을 단어ID에서 원핫 표현으로 변환하면 된다. 이때 다시 한번 각각의 다차원 배열의 형상에 주목하자. 잘 보면, 이 그림에서는 단어ID를 이용했을 때의 맥락의 형상은 (6,2)인데, 이를 원핫표현으로 변환하면 (6,2,7)이 된다. \n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''원핫 표현으로 변환\n",
    "    :param corpus: 단어 ID 목록(1차원 또는 2차원 넘파이 배열)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :return: 원핫 표현(2차원 또는 3차원 넘파이 배열)\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "text = \"You say goodbye and I say hello\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size=1)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 1 0 0 0 0]\n [0 0 1 0 0 0]\n [0 0 0 1 0 0]\n [0 0 0 0 1 0]\n [0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[1 0 0 0 0 0]\n  [0 0 1 0 0 0]]\n\n [[0 1 0 0 0 0]\n  [0 0 0 1 0 0]]\n\n [[0 0 1 0 0 0]\n  [0 0 0 0 1 0]]\n\n [[0 0 0 1 0 0]\n  [0 1 0 0 0 0]]\n\n [[0 0 0 0 1 0]\n  [0 0 0 0 0 1]]]\n"
     ]
    }
   ],
   "source": [
    "print(contexts)"
   ]
  },
  {
   "source": [
    "# 3.4. CBOW 모델 구현\n",
    "\n",
    "[3-19] CBOW모델의 신경망 구현\n",
    "[그림3-19]의 신경망을 SimpleCBOW라는 이름으로 구현할 것이다. SimpleCBOW클래스의 초기화 메서드부터 시작해보자\n",
    "(ch03/simple_cbow.py)\n",
    "\n",
    "\n",
    "<img src = \"./images_equations/fig 3-19.png\" width=400>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "from common.layers import MatMul, SoftmaxWithLoss\n",
    "\n",
    "\n",
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 가중치 초기화, 이때 넘파이 배열의 데이터 타입은astype('f)32비트 부동소수점\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # 계층 생성\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "source": [
    "이것으로 역전파 구현까지 모두 마침. 우리는 이미 각 매개변수의 기울기를 인스턴스 변수 grads에 모아두었다. \n",
    "따라서 foward()매서드를 호출한 다음 backward() 매서드를 실행하는 것만으로 grads리스트의 기울기가 갱신된다. \n",
    "\n",
    "다음은 SimpleCBOW클래스의 학습을 살펴보자"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3.4.1. 학습 코드 구현\n",
    "\n",
    "CBOW 모델의 학습은 일반적인 신경망의 학습과 완전히 같다. \n",
    "학습 데이터를 준비해 신경망에 입력한 다음, 기울기를 구하고 가중치 매개변수를 순서대로 갱신해갈 것이다. \n",
    "\n",
    "1장에서 설명한 Trainer 클래스를 다시 상기해보자. 이번 학습과정을 수행하는데 이 클래스를 이용해보자. \n",
    "아래가 학습을 위한 코드이다 (ch03/train.py)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " 1[s] | 손실 0.63\n",
      "| 에폭 582 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
      "| 에폭 583 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 584 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 585 |  반복 1 / 2 | 시간 1[s] | 손실 0.85\n",
      "| 에폭 586 |  반복 1 / 2 | 시간 1[s] | 손실 0.34\n",
      "| 에폭 587 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 588 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 589 |  반복 1 / 2 | 시간 1[s] | 손실 0.81\n",
      "| 에폭 590 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 591 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 592 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 593 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 594 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 595 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 596 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
      "| 에폭 597 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 598 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 599 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 600 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 601 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 602 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 603 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 604 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 605 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 606 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 607 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 608 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 609 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 610 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
      "| 에폭 611 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 612 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 613 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 614 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 615 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 616 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 617 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 618 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 619 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
      "| 에폭 620 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 621 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 622 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 623 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 624 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 625 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 626 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 627 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 628 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 629 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 630 |  반복 1 / 2 | 시간 1[s] | 손실 0.84\n",
      "| 에폭 631 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
      "| 에폭 632 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 633 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 634 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 635 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 636 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 637 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 638 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 639 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 640 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 641 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 642 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 643 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 644 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 645 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 646 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 647 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 648 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 649 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 650 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 651 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 652 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 653 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 654 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 655 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 656 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 657 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 658 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 659 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 660 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 661 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 662 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 663 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
      "| 에폭 664 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 665 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 666 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 667 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 668 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 669 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 670 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 671 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 672 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 673 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 674 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 675 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 676 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 677 |  반복 1 / 2 | 시간 1[s] | 손실 0.82\n",
      "| 에폭 678 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 679 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 680 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 681 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 682 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 683 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 684 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 685 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 686 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 687 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 688 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 689 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 690 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 691 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 692 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 693 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 694 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 695 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 696 |  반복 1 / 2 | 시간 1[s] | 손실 0.35\n",
      "| 에폭 697 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 698 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 699 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 700 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 701 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 702 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 703 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 704 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 705 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 706 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
      "| 에폭 707 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 708 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 709 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 710 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 711 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 712 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 713 |  반복 1 / 2 | 시간 1[s] | 손실 0.77\n",
      "| 에폭 714 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 715 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 716 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 717 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 718 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 719 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 720 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 721 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 722 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 723 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 724 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 725 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 726 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 727 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 728 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 729 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 730 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 731 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 732 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 733 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 734 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 735 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 736 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 737 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 738 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 739 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 740 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 741 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 742 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 743 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 744 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 745 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 746 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 747 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 748 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 749 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 750 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 751 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 752 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 753 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 754 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 755 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 756 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
      "| 에폭 757 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 758 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 759 |  반복 1 / 2 | 시간 1[s] | 손실 0.40\n",
      "| 에폭 760 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 761 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 762 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 763 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
      "| 에폭 764 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
      "| 에폭 765 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 766 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 767 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 768 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 769 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 770 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 771 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 772 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 773 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 774 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 775 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 776 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 777 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 778 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
      "| 에폭 779 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 780 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 781 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 782 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 783 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 784 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 785 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 786 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
      "| 에폭 787 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 788 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 789 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 790 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 791 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 792 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 793 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 794 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
      "| 에폭 795 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 796 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 797 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 798 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 799 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 800 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
      "| 에폭 801 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 802 |  반복 1 / 2 | 시간 1[s] | 손실 0.32\n",
      "| 에폭 803 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 804 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 805 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 806 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 807 |  반복 1 / 2 | 시간 1[s] | 손실 0.78\n",
      "| 에폭 808 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
      "| 에폭 809 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 810 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 811 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 812 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 813 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 814 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 815 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
      "| 에폭 816 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 817 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 818 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 819 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 820 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 821 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 822 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 823 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 824 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 825 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 826 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 827 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 828 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 829 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
      "| 에폭 830 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
      "| 에폭 831 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 832 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 833 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 834 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 835 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 836 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 837 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 838 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 839 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
      "| 에폭 840 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 841 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 842 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 843 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 844 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 845 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
      "| 에폭 846 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 847 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 848 |  반복 1 / 2 | 시간 1[s] | 손실 0.31\n",
      "| 에폭 849 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 850 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
      "| 에폭 851 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 852 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
      "| 에폭 853 |  반복 1 / 2 | 시간 1[s] | 손실 0.31\n",
      "| 에폭 854 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 855 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
      "| 에폭 856 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 857 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 858 |  반복 1 / 2 | 시간 1[s] | 손실 0.31\n",
      "| 에폭 859 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 860 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 861 |  반복 1 / 2 | 시간 1[s] | 손실 0.36\n",
      "| 에폭 862 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
      "| 에폭 863 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 864 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
      "| 에폭 865 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
      "| 에폭 866 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 867 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 868 |  반복 1 / 2 | 시간 1[s] | 손실 0.65\n",
      "| 에폭 869 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 870 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 871 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 872 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 873 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
      "| 에폭 874 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 875 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
      "| 에폭 876 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 877 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
      "| 에폭 878 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 879 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 880 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 881 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
      "| 에폭 882 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 883 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
      "| 에폭 884 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
      "| 에폭 885 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 886 |  반복 1 / 2 | 시간 1[s] | 손실 0.35\n",
      "| 에폭 887 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
      "| 에폭 888 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 889 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 890 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 891 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
      "| 에폭 892 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 893 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 894 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 895 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
      "| 에폭 896 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 897 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 898 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
      "| 에폭 899 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
      "| 에폭 900 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
      "| 에폭 901 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
      "| 에폭 902 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 903 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 904 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 905 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
      "| 에폭 906 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 907 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 908 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 909 |  반복 1 / 2 | 시간 1[s] | 손실 0.41\n",
      "| 에폭 910 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
      "| 에폭 911 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
      "| 에폭 912 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
      "| 에폭 913 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 914 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
      "| 에폭 915 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
      "| 에폭 916 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
      "| 에폭 917 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
      "| 에폭 918 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 919 |  반복 1 / 2 | 시간 2[s] | 손실 0.58\n",
      "| 에폭 920 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 921 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 922 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 923 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 924 |  반복 1 / 2 | 시간 2[s] | 손실 0.63\n",
      "| 에폭 925 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 926 |  반복 1 / 2 | 시간 2[s] | 손실 0.69\n",
      "| 에폭 927 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 928 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 929 |  반복 1 / 2 | 시간 2[s] | 손실 0.60\n",
      "| 에폭 930 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 931 |  반복 1 / 2 | 시간 2[s] | 손실 0.58\n",
      "| 에폭 932 |  반복 1 / 2 | 시간 2[s] | 손실 0.62\n",
      "| 에폭 933 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 934 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 935 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 936 |  반복 1 / 2 | 시간 2[s] | 손실 0.57\n",
      "| 에폭 937 |  반복 1 / 2 | 시간 2[s] | 손실 0.34\n",
      "| 에폭 938 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 939 |  반복 1 / 2 | 시간 2[s] | 손실 0.73\n",
      "| 에폭 940 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 941 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 942 |  반복 1 / 2 | 시간 2[s] | 손실 0.46\n",
      "| 에폭 943 |  반복 1 / 2 | 시간 2[s] | 손실 0.69\n",
      "| 에폭 944 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 945 |  반복 1 / 2 | 시간 2[s] | 손실 0.61\n",
      "| 에폭 946 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 947 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 948 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 949 |  반복 1 / 2 | 시간 2[s] | 손실 0.46\n",
      "| 에폭 950 |  반복 1 / 2 | 시간 2[s] | 손실 0.61\n",
      "| 에폭 951 |  반복 1 / 2 | 시간 2[s] | 손실 0.47\n",
      "| 에폭 952 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 953 |  반복 1 / 2 | 시간 2[s] | 손실 0.61\n",
      "| 에폭 954 |  반복 1 / 2 | 시간 2[s] | 손실 0.40\n",
      "| 에폭 955 |  반복 1 / 2 | 시간 2[s] | 손실 0.60\n",
      "| 에폭 956 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 957 |  반복 1 / 2 | 시간 2[s] | 손실 0.32\n",
      "| 에폭 958 |  반복 1 / 2 | 시간 2[s] | 손실 0.68\n",
      "| 에폭 959 |  반복 1 / 2 | 시간 2[s] | 손실 0.53\n",
      "| 에폭 960 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 961 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 962 |  반복 1 / 2 | 시간 2[s] | 손실 0.64\n",
      "| 에폭 963 |  반복 1 / 2 | 시간 2[s] | 손실 0.36\n",
      "| 에폭 964 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 965 |  반복 1 / 2 | 시간 2[s] | 손실 0.71\n",
      "| 에폭 966 |  반복 1 / 2 | 시간 2[s] | 손실 0.43\n",
      "| 에폭 967 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 968 |  반복 1 / 2 | 시간 2[s] | 손실 0.50\n",
      "| 에폭 969 |  반복 1 / 2 | 시간 2[s] | 손실 0.35\n",
      "| 에폭 970 |  반복 1 / 2 | 시간 2[s] | 손실 0.64\n",
      "| 에폭 971 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 972 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 973 |  반복 1 / 2 | 시간 2[s] | 손실 0.60\n",
      "| 에폭 974 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 975 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 976 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 977 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 978 |  반복 1 / 2 | 시간 2[s] | 손실 0.42\n",
      "| 에폭 979 |  반복 1 / 2 | 시간 2[s] | 손실 0.63\n",
      "| 에폭 980 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 981 |  반복 1 / 2 | 시간 2[s] | 손실 0.68\n",
      "| 에폭 982 |  반복 1 / 2 | 시간 2[s] | 손실 0.31\n",
      "| 에폭 983 |  반복 1 / 2 | 시간 2[s] | 손실 0.58\n",
      "| 에폭 984 |  반복 1 / 2 | 시간 2[s] | 손실 0.52\n",
      "| 에폭 985 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 986 |  반복 1 / 2 | 시간 2[s] | 손실 0.60\n",
      "| 에폭 987 |  반복 1 / 2 | 시간 2[s] | 손실 0.38\n",
      "| 에폭 988 |  반복 1 / 2 | 시간 2[s] | 손실 0.60\n",
      "| 에폭 989 |  반복 1 / 2 | 시간 2[s] | 손실 0.28\n",
      "| 에폭 990 |  반복 1 / 2 | 시간 2[s] | 손실 0.70\n",
      "| 에폭 991 |  반복 1 / 2 | 시간 2[s] | 손실 0.45\n",
      "| 에폭 992 |  반복 1 / 2 | 시간 2[s] | 손실 0.51\n",
      "| 에폭 993 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 994 |  반복 1 / 2 | 시간 2[s] | 손실 0.56\n",
      "| 에폭 995 |  반복 1 / 2 | 시간 2[s] | 손실 0.44\n",
      "| 에폭 996 |  반복 1 / 2 | 시간 2[s] | 손실 0.49\n",
      "| 에폭 997 |  반복 1 / 2 | 시간 2[s] | 손실 0.39\n",
      "| 에폭 998 |  반복 1 / 2 | 시간 2[s] | 손실 0.55\n",
      "| 에폭 999 |  반복 1 / 2 | 시간 2[s] | 손실 0.48\n",
      "| 에폭 1000 |  반복 1 / 2 | 시간 2[s] | 손실 0.56\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/textpath.py:90: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/textpath.py:90: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/textpath.py:203: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/textpath.py:203: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/textpath.py:90: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/textpath.py:90: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=LOAD_NO_HINTING)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/textpath.py:203: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/textpath.py:203: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  glyph = font.load_char(ccode, flags=LOAD_NO_HINTING)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 48152 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 48373 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:211: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 49552 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:180: RuntimeWarning: Glyph 49892 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"262.522666pt\" version=\"1.1\" viewBox=\"0 0 392.14375 262.522666\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.522666 \nL 392.14375 262.522666 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 224.966416 \nL 384.94375 224.966416 \nL 384.94375 7.526416 \nL 50.14375 7.526416 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m8cff282c34\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#m8cff282c34\" y=\"224.966416\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(62.180682 239.564853)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"126.295593\" xlink:href=\"#m8cff282c34\" y=\"224.966416\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 200 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(116.751843 239.564853)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.229254\" xlink:href=\"#m8cff282c34\" y=\"224.966416\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 400 -->\n      <defs>\n       <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n      </defs>\n      <g transform=\"translate(177.685504 239.564853)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.162915\" xlink:href=\"#m8cff282c34\" y=\"224.966416\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 600 -->\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <g transform=\"translate(238.619165 239.564853)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-54\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"309.096576\" xlink:href=\"#m8cff282c34\" y=\"224.966416\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 800 -->\n      <defs>\n       <path d=\"M 31.78125 34.625 \nQ 24.75 34.625 20.71875 30.859375 \nQ 16.703125 27.09375 16.703125 20.515625 \nQ 16.703125 13.921875 20.71875 10.15625 \nQ 24.75 6.390625 31.78125 6.390625 \nQ 38.8125 6.390625 42.859375 10.171875 \nQ 46.921875 13.96875 46.921875 20.515625 \nQ 46.921875 27.09375 42.890625 30.859375 \nQ 38.875 34.625 31.78125 34.625 \nz\nM 21.921875 38.8125 \nQ 15.578125 40.375 12.03125 44.71875 \nQ 8.5 49.078125 8.5 55.328125 \nQ 8.5 64.0625 14.71875 69.140625 \nQ 20.953125 74.21875 31.78125 74.21875 \nQ 42.671875 74.21875 48.875 69.140625 \nQ 55.078125 64.0625 55.078125 55.328125 \nQ 55.078125 49.078125 51.53125 44.71875 \nQ 48 40.375 41.703125 38.8125 \nQ 48.828125 37.15625 52.796875 32.3125 \nQ 56.78125 27.484375 56.78125 20.515625 \nQ 56.78125 9.90625 50.3125 4.234375 \nQ 43.84375 -1.421875 31.78125 -1.421875 \nQ 19.734375 -1.421875 13.25 4.234375 \nQ 6.78125 9.90625 6.78125 20.515625 \nQ 6.78125 27.484375 10.78125 32.3125 \nQ 14.796875 37.15625 21.921875 38.8125 \nz\nM 18.3125 54.390625 \nQ 18.3125 48.734375 21.84375 45.5625 \nQ 25.390625 42.390625 31.78125 42.390625 \nQ 38.140625 42.390625 41.71875 45.5625 \nQ 45.3125 48.734375 45.3125 54.390625 \nQ 45.3125 60.0625 41.71875 63.234375 \nQ 38.140625 66.40625 31.78125 66.40625 \nQ 25.390625 66.40625 21.84375 63.234375 \nQ 18.3125 60.0625 18.3125 54.390625 \nz\n\" id=\"DejaVuSans-56\"/>\n      </defs>\n      <g transform=\"translate(299.552826 239.564853)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-56\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"370.030236\" xlink:href=\"#m8cff282c34\" y=\"224.966416\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 1000 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(357.305236 239.564853)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- 반복 (x20) -->\n     <defs>\n      <path d=\"M 4.984375 -17.671875 \nL 4.984375 70.515625 \nL 54.984375 70.515625 \nL 54.984375 -17.671875 \nz\nM 10.59375 -12.109375 \nL 49.421875 -12.109375 \nL 49.421875 64.890625 \nL 10.59375 64.890625 \nz\n\" id=\"DejaVuSans-48152\"/>\n      <path d=\"M 4.984375 -17.671875 \nL 4.984375 70.515625 \nL 54.984375 70.515625 \nL 54.984375 -17.671875 \nz\nM 10.59375 -12.109375 \nL 49.421875 -12.109375 \nL 49.421875 64.890625 \nL 10.59375 64.890625 \nz\n\" id=\"DejaVuSans-48373\"/>\n      <path id=\"DejaVuSans-32\"/>\n      <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n      <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n      <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n     </defs>\n     <g transform=\"translate(196.729688 253.242978)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-48152\"/>\n      <use x=\"60.009766\" xlink:href=\"#DejaVuSans-48373\"/>\n      <use x=\"120.019531\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"151.806641\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"190.820312\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"250\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"313.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"377.246094\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m0ed8cd284a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m0ed8cd284a\" y=\"218.6642\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.25 -->\n      <defs>\n       <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(20.878125 222.463419)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m0ed8cd284a\" y=\"188.997774\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.50 -->\n      <g transform=\"translate(20.878125 192.796993)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m0ed8cd284a\" y=\"159.331348\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.75 -->\n      <defs>\n       <path d=\"M 8.203125 72.90625 \nL 55.078125 72.90625 \nL 55.078125 68.703125 \nL 28.609375 0 \nL 18.3125 0 \nL 43.21875 64.59375 \nL 8.203125 64.59375 \nz\n\" id=\"DejaVuSans-55\"/>\n      </defs>\n      <g transform=\"translate(20.878125 163.130567)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m0ed8cd284a\" y=\"129.664922\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 1.00 -->\n      <g transform=\"translate(20.878125 133.464141)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m0ed8cd284a\" y=\"99.998496\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 1.25 -->\n      <g transform=\"translate(20.878125 103.797715)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m0ed8cd284a\" y=\"70.332071\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 1.50 -->\n      <g transform=\"translate(20.878125 74.131289)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m0ed8cd284a\" y=\"40.665645\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 1.75 -->\n      <g transform=\"translate(20.878125 44.464863)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-55\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m0ed8cd284a\" y=\"10.999219\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- 2.00 -->\n      <g transform=\"translate(20.878125 14.798437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- 손실 -->\n     <defs>\n      <path d=\"M 4.984375 -17.671875 \nL 4.984375 70.515625 \nL 54.984375 70.515625 \nL 54.984375 -17.671875 \nz\nM 10.59375 -12.109375 \nL 49.421875 -12.109375 \nL 49.421875 64.890625 \nL 10.59375 64.890625 \nz\n\" id=\"DejaVuSans-49552\"/>\n      <path d=\"M 4.984375 -17.671875 \nL 4.984375 70.515625 \nL 54.984375 70.515625 \nL 54.984375 -17.671875 \nz\nM 10.59375 -12.109375 \nL 49.421875 -12.109375 \nL 49.421875 64.890625 \nL 10.59375 64.890625 \nz\n\" id=\"DejaVuSans-49892\"/>\n     </defs>\n     <g transform=\"translate(14.798438 122.247978)rotate(-90)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-49552\"/>\n      <use x=\"60.009766\" xlink:href=\"#DejaVuSans-49892\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p1e57033364)\" d=\"M 65.361932 17.42721 \nL 66.580605 17.45802 \nL 68.103947 17.550425 \nL 69.627288 17.686089 \nL 70.845961 17.919308 \nL 71.15063 17.876835 \nL 71.455298 18.02255 \nL 71.759966 18.016258 \nL 72.064635 18.184991 \nL 72.978639 18.36801 \nL 74.806649 19.000402 \nL 75.415986 19.36741 \nL 75.720654 19.28985 \nL 76.025322 19.681909 \nL 76.329991 19.56116 \nL 76.634659 19.993815 \nL 76.939327 19.928506 \nL 77.243996 20.160486 \nL 77.548664 20.203906 \nL 77.853332 20.686355 \nL 78.767337 21.275611 \nL 79.072006 20.964469 \nL 79.376674 21.7388 \nL 79.681342 21.705598 \nL 79.98601 22.245157 \nL 80.290679 22.536773 \nL 80.595347 22.602094 \nL 80.900015 22.453268 \nL 81.509352 23.620871 \nL 81.81402 23.466099 \nL 82.118689 24.135862 \nL 82.423357 24.198762 \nL 82.728025 24.837056 \nL 83.032693 24.070638 \nL 83.337362 25.10827 \nL 83.64203 25.488642 \nL 83.946698 25.574608 \nL 84.860703 27.183238 \nL 85.165372 26.507648 \nL 85.47004 28.100909 \nL 85.774708 27.473101 \nL 86.079377 28.662916 \nL 86.384045 28.198983 \nL 86.688713 28.083325 \nL 86.993381 30.806123 \nL 87.29805 28.306619 \nL 87.602718 31.517515 \nL 87.907386 29.956415 \nL 88.212055 30.471649 \nL 88.516723 31.552391 \nL 88.821391 34.238599 \nL 89.12606 31.900985 \nL 89.430728 32.763736 \nL 89.735396 31.951124 \nL 90.040064 34.255603 \nL 90.649401 33.057738 \nL 90.954069 36.256753 \nL 91.258738 35.477234 \nL 91.563406 35.93894 \nL 91.868074 36.63471 \nL 92.172743 34.75984 \nL 92.477411 40.227968 \nL 93.086748 37.35248 \nL 93.391416 38.34779 \nL 93.696084 40.924306 \nL 94.000752 37.405747 \nL 94.610089 43.596318 \nL 94.914757 38.533983 \nL 95.219426 42.059072 \nL 95.524094 41.400135 \nL 95.828762 45.079577 \nL 96.133431 40.766557 \nL 96.438099 48.20641 \nL 96.742767 43.470788 \nL 97.047436 42.177477 \nL 97.352104 46.579708 \nL 97.656772 47.812942 \nL 97.96144 47.753539 \nL 98.266109 46.569815 \nL 98.570777 48.990589 \nL 98.875445 45.94108 \nL 99.180114 54.452074 \nL 99.484782 47.080699 \nL 99.78945 52.860847 \nL 100.094119 46.693741 \nL 100.398787 53.030915 \nL 100.703455 53.34305 \nL 101.008123 50.051782 \nL 101.312792 54.529925 \nL 101.61746 52.675922 \nL 101.922128 52.351861 \nL 102.226797 59.517025 \nL 102.531465 54.215508 \nL 102.836133 56.157678 \nL 103.140802 55.369265 \nL 103.44547 60.804399 \nL 103.750138 54.495173 \nL 104.054807 58.757842 \nL 104.359475 60.423362 \nL 104.664143 56.091757 \nL 104.968811 67.613827 \nL 105.27348 56.110817 \nL 105.578148 66.561963 \nL 105.882816 59.834682 \nL 106.187485 61.454141 \nL 106.492153 59.586956 \nL 106.796821 66.51528 \nL 107.10149 66.003285 \nL 107.406158 59.639873 \nL 107.710826 67.376136 \nL 108.015494 64.825367 \nL 108.320163 76.929858 \nL 108.624831 66.23687 \nL 108.929499 65.409152 \nL 109.234168 70.343509 \nL 109.538836 72.462661 \nL 109.843504 65.58668 \nL 110.148173 65.965662 \nL 110.452841 76.015203 \nL 110.757509 71.907153 \nL 111.062178 70.823554 \nL 111.366846 73.062604 \nL 111.671514 68.925377 \nL 111.976182 79.30663 \nL 112.280851 76.303148 \nL 112.585519 72.134767 \nL 113.194856 83.292708 \nL 113.499524 72.208145 \nL 113.804192 76.241058 \nL 114.413529 80.398911 \nL 114.718197 79.629179 \nL 115.022865 82.189537 \nL 115.327534 77.449763 \nL 115.93687 87.420827 \nL 116.241539 75.064258 \nL 116.546207 90.705724 \nL 116.850875 70.437923 \nL 117.155544 90.156915 \nL 117.460212 86.638295 \nL 117.76488 91.194911 \nL 118.069549 81.645189 \nL 118.374217 84.44149 \nL 118.678885 88.932032 \nL 118.983553 85.826772 \nL 119.59289 88.447804 \nL 119.897558 97.856811 \nL 120.202227 82.680507 \nL 120.506895 92.339325 \nL 120.811563 81.577053 \nL 121.116232 97.705843 \nL 121.4209 92.368414 \nL 121.725568 92.311005 \nL 122.030236 99.824957 \nL 122.334905 83.674051 \nL 122.639573 94.192199 \nL 122.944241 98.390152 \nL 123.24891 96.034617 \nL 123.553578 91.20749 \nL 123.858246 100.850538 \nL 124.162915 89.927752 \nL 124.772251 105.719876 \nL 125.07692 99.330399 \nL 125.381588 89.81576 \nL 125.686256 114.21398 \nL 125.990924 91.578779 \nL 126.295593 109.484463 \nL 126.600261 90.931665 \nL 126.904929 101.753357 \nL 127.209598 101.844805 \nL 127.514266 95.610001 \nL 127.818934 111.992866 \nL 128.123603 100.94019 \nL 128.428271 96.399284 \nL 128.732939 111.443532 \nL 129.037607 114.104195 \nL 129.342276 97.757759 \nL 129.646944 96.194524 \nL 130.256281 115.947614 \nL 130.560949 105.107055 \nL 130.865617 109.594335 \nL 131.170286 108.022938 \nL 131.474954 115.547912 \nL 131.779622 99.247135 \nL 132.084291 103.816655 \nL 132.388959 117.379917 \nL 132.693627 115.571207 \nL 132.998295 103.275686 \nL 133.607632 119.032247 \nL 133.9123 102.144823 \nL 134.216969 113.870025 \nL 134.521637 120.906004 \nL 134.826305 103.374101 \nL 135.435642 122.773054 \nL 135.74031 105.511536 \nL 136.044979 114.748065 \nL 136.349647 113.970571 \nL 136.654315 125.36526 \nL 136.958983 108.344304 \nL 137.263652 115.114614 \nL 137.56832 118.047157 \nL 137.872988 115.387284 \nL 138.177657 125.908373 \nL 138.482325 109.620051 \nL 138.786993 120.021012 \nL 139.091662 117.031803 \nL 139.39633 128.846363 \nL 139.700998 119.47141 \nL 140.005666 117.811544 \nL 140.310335 112.487297 \nL 140.615003 112.490658 \nL 140.919671 128.637214 \nL 141.22434 111.662687 \nL 141.529008 131.360696 \nL 141.833676 114.333407 \nL 142.138345 132.019414 \nL 142.443013 119.148392 \nL 142.747681 125.216394 \nL 143.05235 123.063186 \nL 143.357018 133.759158 \nL 143.661686 114.600079 \nL 143.966354 123.425746 \nL 144.271023 127.281663 \nL 144.575691 121.375761 \nL 144.880359 137.458877 \nL 145.185028 115.432561 \nL 145.489696 135.004579 \nL 145.794364 109.265421 \nL 146.099033 142.851954 \nL 146.403701 120.177319 \nL 147.013037 134.512406 \nL 147.317706 110.99999 \nL 147.622374 136.242357 \nL 147.927042 128.818756 \nL 148.536379 132.163165 \nL 148.841047 126.822986 \nL 149.145716 132.817982 \nL 149.450384 127.403089 \nL 149.755052 138.458426 \nL 150.059721 130.889719 \nL 150.364389 113.98487 \nL 150.669057 140.742741 \nL 151.278394 123.811145 \nL 151.88773 140.958145 \nL 152.192399 128.020426 \nL 152.497067 149.398477 \nL 152.801735 125.747404 \nL 153.106404 134.097536 \nL 153.411072 121.728353 \nL 153.71574 146.279314 \nL 154.020408 131.197553 \nL 154.325077 134.805746 \nL 154.629745 147.683416 \nL 154.934413 131.39252 \nL 155.239082 127.538284 \nL 155.54375 139.918786 \nL 155.848418 136.716663 \nL 156.153087 136.496487 \nL 156.457755 132.691244 \nL 156.762423 137.149472 \nL 157.067092 149.405837 \nL 157.37176 117.401927 \nL 157.676428 146.169014 \nL 157.981096 142.260918 \nL 158.285765 146.796217 \nL 158.590433 117.813514 \nL 158.895101 151.990488 \nL 159.19977 146.524529 \nL 159.504438 132.193906 \nL 159.809106 139.096877 \nL 160.113775 139.964547 \nL 160.418443 135.299035 \nL 160.723111 145.842727 \nL 161.027779 132.826286 \nL 161.332448 143.527311 \nL 161.637116 133.996188 \nL 161.941784 153.934371 \nL 162.246453 148.797435 \nL 162.551121 134.897375 \nL 162.855789 136.111409 \nL 163.160458 148.782859 \nL 163.465126 142.260557 \nL 163.769794 150.002052 \nL 164.074463 143.071705 \nL 164.379131 128.412871 \nL 164.683799 151.362542 \nL 164.988467 137.091851 \nL 165.597804 150.652754 \nL 165.902472 137.517119 \nL 166.207141 151.648385 \nL 166.511809 145.182134 \nL 166.816477 143.214785 \nL 167.121146 145.359236 \nL 167.425814 140.102243 \nL 167.730482 145.448031 \nL 168.03515 145.664651 \nL 168.339819 146.325275 \nL 168.644487 148.640997 \nL 168.949155 157.852638 \nL 169.253824 133.159515 \nL 169.86316 152.504609 \nL 170.167829 147.804189 \nL 170.472497 155.447674 \nL 171.081834 136.736077 \nL 171.386502 162.263738 \nL 171.69117 147.65369 \nL 171.995838 140.634336 \nL 172.300507 148.786335 \nL 172.605175 149.330874 \nL 172.909843 150.104524 \nL 173.214512 140.46936 \nL 173.51918 163.535671 \nL 173.823848 157.507129 \nL 174.128517 136.321941 \nL 174.433185 164.161351 \nL 174.737853 142.41527 \nL 175.042521 154.042793 \nL 175.34719 136.694506 \nL 175.651858 158.638367 \nL 175.956526 161.619453 \nL 176.565863 137.378707 \nL 176.870531 155.828623 \nL 177.1752 158.115385 \nL 177.479868 149.496713 \nL 177.784536 148.555778 \nL 178.089205 148.21878 \nL 178.393873 170.908317 \nL 178.698541 134.797334 \nL 179.003209 160.585458 \nL 179.307878 155.474023 \nL 179.612546 143.421457 \nL 179.917214 161.099758 \nL 180.221883 160.125054 \nL 180.526551 139.785147 \nL 180.831219 168.379888 \nL 181.135888 139.543772 \nL 181.440556 164.246652 \nL 181.745224 162.149284 \nL 182.049893 140.462547 \nL 182.354561 154.582449 \nL 182.659229 152.321498 \nL 182.963897 172.226824 \nL 183.268566 155.560898 \nL 183.573234 155.244548 \nL 183.877902 148.056059 \nL 184.182571 149.079072 \nL 184.791907 158.674762 \nL 185.096576 156.234284 \nL 185.401244 156.429885 \nL 185.705912 154.002068 \nL 186.01058 166.049121 \nL 186.315249 162.258965 \nL 186.619917 157.055423 \nL 187.229254 162.903403 \nL 187.533922 151.945759 \nL 187.83859 149.890194 \nL 188.143259 156.521053 \nL 188.447927 152.267674 \nL 188.752595 172.79384 \nL 189.057264 149.334728 \nL 189.361932 167.020982 \nL 189.6666 164.729227 \nL 189.971268 150.946483 \nL 190.275937 153.259184 \nL 190.580605 164.630942 \nL 191.189942 153.004908 \nL 191.49461 150.910933 \nL 191.799278 153.177183 \nL 192.103947 173.743367 \nL 192.408615 159.831666 \nL 192.713283 153.298301 \nL 193.017951 181.574673 \nL 193.32262 138.645423 \nL 193.627288 174.494883 \nL 193.931956 160.510838 \nL 194.236625 153.938791 \nL 194.541293 153.064887 \nL 194.845961 167.942568 \nL 195.15063 175.857141 \nL 195.455298 138.638585 \nL 195.759966 169.096495 \nL 196.064635 153.599836 \nL 196.369303 154.404514 \nL 196.673971 184.080386 \nL 196.978639 139.268074 \nL 197.283308 169.491749 \nL 197.587976 169.283397 \nL 197.892644 154.818142 \nL 198.197313 169.540688 \nL 198.501981 169.951275 \nL 198.806649 155.017051 \nL 199.111318 155.226921 \nL 199.415986 177.697661 \nL 199.720654 140.100902 \nL 200.025322 170.823917 \nL 200.329991 170.447198 \nL 200.939327 155.971833 \nL 201.243996 178.157213 \nL 201.548664 156.23883 \nL 201.853332 156.273716 \nL 202.158001 177.945822 \nL 202.462669 148.927221 \nL 202.767337 178.974915 \nL 203.072006 164.114375 \nL 203.376674 156.803922 \nL 203.681342 156.779394 \nL 203.98601 180.194894 \nL 204.290679 164.597024 \nL 204.595347 163.826437 \nL 204.900015 158.229977 \nL 205.204684 163.857985 \nL 205.509352 158.326758 \nL 205.81402 156.621774 \nL 206.118689 181.147016 \nL 206.423357 173.029527 \nL 206.728025 149.105384 \nL 207.032693 165.462541 \nL 207.337362 173.183167 \nL 207.64203 157.941122 \nL 207.946698 182.228418 \nL 208.251367 173.374007 \nL 208.556035 158.666269 \nL 208.860703 158.666289 \nL 209.165372 173.548273 \nL 209.47004 166.42651 \nL 209.774708 142.170949 \nL 210.079377 190.725544 \nL 210.384045 149.760858 \nL 210.688713 183.506796 \nL 210.993381 166.785749 \nL 211.29805 159.053357 \nL 211.602718 157.822478 \nL 211.907386 174.705586 \nL 212.212055 174.393327 \nL 212.516723 154.061917 \nL 212.821391 180.430818 \nL 213.12606 152.258614 \nL 213.430728 177.414055 \nL 213.735396 157.646656 \nL 214.040064 177.193379 \nL 214.344733 165.594079 \nL 214.649401 175.677851 \nL 214.954069 170.18103 \nL 215.258738 158.367711 \nL 215.563406 169.948458 \nL 215.868074 158.574981 \nL 216.172743 177.92011 \nL 216.477411 151.352381 \nL 216.782079 175.796723 \nL 217.086748 178.777608 \nL 217.391416 158.239864 \nL 217.696084 171.255072 \nL 218.000752 166.204755 \nL 218.305421 183.995785 \nL 218.610089 171.588027 \nL 218.914757 151.231749 \nL 219.219426 171.70075 \nL 219.524094 184.418517 \nL 219.828762 169.14525 \nL 220.133431 162.144177 \nL 220.438099 161.623659 \nL 221.047436 172.341357 \nL 221.352104 174.621336 \nL 221.656772 161.850438 \nL 222.266109 180.142563 \nL 222.570777 177.802371 \nL 223.180114 159.752603 \nL 223.78945 180.677357 \nL 224.094119 144.715949 \nL 224.398787 196.02292 \nL 224.703455 163.191492 \nL 225.008123 162.596643 \nL 225.312792 167.373085 \nL 225.922128 188.535744 \nL 226.226797 155.660604 \nL 226.531465 168.252212 \nL 226.836133 163.055252 \nL 227.140802 171.057222 \nL 227.44547 171.083345 \nL 227.750138 179.038561 \nL 228.054807 181.86652 \nL 228.359475 152.746827 \nL 228.664143 186.430566 \nL 228.968811 167.785337 \nL 229.27348 178.721539 \nL 229.578148 156.401125 \nL 229.882816 175.972619 \nL 230.187485 167.392006 \nL 230.492153 178.992671 \nL 230.796821 179.801578 \nL 231.10149 179.086114 \nL 231.406158 145.910342 \nL 232.015494 191.016027 \nL 232.624831 157.078922 \nL 232.929499 187.416129 \nL 233.234168 153.390939 \nL 233.538836 191.413012 \nL 233.843504 153.370405 \nL 234.148173 179.741739 \nL 234.452841 165.427922 \nL 234.757509 191.620273 \nL 235.062178 165.557241 \nL 235.366846 179.987405 \nL 235.671514 146.458545 \nL 235.976182 195.154062 \nL 236.280851 157.83372 \nL 236.585519 180.909504 \nL 236.890187 176.362897 \nL 237.194856 173.89169 \nL 237.499524 172.415138 \nL 237.804192 174.005356 \nL 238.108861 180.514371 \nL 238.413529 173.338944 \nL 238.718197 173.409669 \nL 239.022865 169.274786 \nL 239.327534 162.428806 \nL 239.632202 188.709069 \nL 239.93687 165.703919 \nL 240.241539 170.188653 \nL 240.546207 165.756292 \nL 240.850875 185.277309 \nL 241.155544 162.37785 \nL 241.460212 166.708447 \nL 241.76488 200.501382 \nL 242.374217 154.527869 \nL 242.678885 178.499859 \nL 242.983553 189.241401 \nL 243.288222 147.533377 \nL 243.59289 208.089953 \nL 243.897558 162.688099 \nL 244.202227 174.388733 \nL 244.506895 152.07845 \nL 244.811563 194.068627 \nL 245.116232 177.216054 \nL 246.030236 170.138388 \nL 246.334905 182.698732 \nL 246.639573 159.633507 \nL 246.944241 186.524138 \nL 247.24891 163.160389 \nL 247.553578 167.718371 \nL 248.162915 194.707054 \nL 248.772251 160.13143 \nL 249.07692 185.444155 \nL 249.381588 160.137518 \nL 249.990924 195.015311 \nL 250.295593 155.598212 \nL 250.904929 202.356491 \nL 251.209598 155.680557 \nL 251.514266 180.310205 \nL 251.818934 183.520551 \nL 252.123603 163.65195 \nL 252.428271 174.940194 \nL 252.732939 180.549189 \nL 253.037607 176.500127 \nL 253.342276 178.165772 \nL 253.646944 153.598893 \nL 253.951612 187.957994 \nL 254.256281 175.889894 \nL 254.865617 176.061047 \nL 255.170286 186.082896 \nL 255.474954 181.057874 \nL 255.779622 164.093663 \nL 256.084291 168.359712 \nL 256.388959 189.058902 \nL 256.693627 183.461481 \nL 256.998295 149.206857 \nL 257.302964 195.678829 \nL 257.607632 169.305159 \nL 257.9123 186.403518 \nL 258.216969 173.704494 \nL 258.521637 184.384283 \nL 258.826305 164.438049 \nL 259.130974 175.97136 \nL 259.435642 169.53559 \nL 259.74031 176.708252 \nL 260.044979 188.997407 \nL 260.349647 184.626881 \nL 260.654315 164.613748 \nL 261.263652 189.210193 \nL 261.56832 164.051735 \nL 262.482325 182.262392 \nL 262.786993 172.014488 \nL 263.091662 189.459264 \nL 263.39633 177.843876 \nL 263.700998 172.120189 \nL 264.005666 181.86552 \nL 264.310335 177.349413 \nL 264.615003 185.147061 \nL 264.919671 165.084415 \nL 265.22434 169.68616 \nL 265.529008 170.334527 \nL 265.833676 184.711364 \nL 266.138345 170.417625 \nL 266.443013 192.514499 \nL 266.747681 162.76269 \nL 267.05235 197.827593 \nL 267.357018 171.957148 \nL 267.661686 190.744825 \nL 267.966354 172.576681 \nL 268.271023 183.105283 \nL 268.575691 177.90459 \nL 268.880359 157.683896 \nL 269.185028 185.726735 \nL 269.489696 170.278046 \nL 269.794364 191.049476 \nL 270.099033 180.008118 \nL 270.403701 163.207109 \nL 270.708369 193.07319 \nL 271.013037 190.673255 \nL 271.317706 150.777549 \nL 271.622374 198.49874 \nL 271.927042 158.10195 \nL 272.231711 198.575416 \nL 272.536379 173.026487 \nL 272.841047 190.981323 \nL 273.450384 158.258451 \nL 273.755052 198.722321 \nL 274.059721 173.241359 \nL 274.364389 178.615691 \nL 274.669057 169.085954 \nL 274.973725 178.699239 \nL 275.278394 173.354594 \nL 275.583062 198.973254 \nL 275.88773 178.723317 \nL 276.192399 166.253775 \nL 276.497067 176.665978 \nL 276.801735 166.268756 \nL 277.106404 206.377191 \nL 277.411072 166.325572 \nL 277.71574 184.080609 \nL 278.020408 179.405571 \nL 278.325077 186.331245 \nL 278.934413 171.907965 \nL 279.239082 180.983534 \nL 279.54375 169.865523 \nL 279.848418 194.046464 \nL 280.153087 159.083512 \nL 280.457755 179.344054 \nL 280.762423 184.781098 \nL 281.067092 194.156282 \nL 281.37176 172.196429 \nL 281.676428 186.743829 \nL 281.981096 186.7469 \nL 282.285765 157.529435 \nL 282.895101 187.083776 \nL 283.19977 172.194647 \nL 283.504438 187.135639 \nL 283.809106 172.290138 \nL 284.113775 199.964982 \nL 284.418443 174.241104 \nL 284.723111 165.113864 \nL 285.027779 185.378363 \nL 285.332448 174.396493 \nL 285.637116 200.084015 \nL 285.941784 172.58349 \nL 286.246453 174.606673 \nL 286.551121 172.656622 \nL 286.855789 180.082805 \nL 287.160458 194.71865 \nL 287.465126 165.497786 \nL 287.769794 193.058782 \nL 288.074463 181.985281 \nL 288.379131 165.598719 \nL 288.683799 194.922137 \nL 288.988467 178.506088 \nL 289.293136 182.160177 \nL 289.597804 173.034058 \nL 289.902472 185.954065 \nL 290.207141 167.628478 \nL 290.511809 180.421443 \nL 290.816477 186.126286 \nL 291.425814 178.869915 \nL 291.730482 195.179814 \nL 292.03515 175.09637 \nL 292.339819 186.240193 \nL 292.644487 166.151435 \nL 292.949155 189.748675 \nL 293.253824 186.322675 \nL 293.558492 180.838968 \nL 293.86316 167.961128 \nL 294.167829 186.592273 \nL 294.777165 175.240177 \nL 295.081834 193.928658 \nL 295.386502 168.104859 \nL 295.69117 181.07517 \nL 295.995838 166.583403 \nL 296.300507 201.436958 \nL 296.605175 168.234583 \nL 297.214512 194.184289 \nL 297.51918 160.984759 \nL 297.823848 201.618506 \nL 298.128517 181.356913 \nL 298.433185 175.778243 \nL 298.737853 194.057182 \nL 299.34719 168.51279 \nL 299.956526 190.312958 \nL 300.261195 174.199251 \nL 300.565863 187.598619 \nL 301.1752 175.675527 \nL 301.479868 174.751064 \nL 301.784536 174.355223 \nL 302.089205 201.730749 \nL 302.393873 169.29379 \nL 302.698541 188.764826 \nL 303.003209 174.974301 \nL 303.307878 187.545825 \nL 303.612546 183.20861 \nL 303.917214 180.725966 \nL 304.221883 176.412179 \nL 304.526551 201.936784 \nL 304.831219 174.665046 \nL 305.135888 176.528357 \nL 305.440556 195.205884 \nL 306.049893 169.184485 \nL 306.354561 181.044587 \nL 306.659229 176.708387 \nL 306.963897 202.135501 \nL 307.268566 182.380484 \nL 307.573234 189.87471 \nL 307.877902 169.376477 \nL 308.182571 181.286413 \nL 308.487239 169.460845 \nL 308.791907 202.353636 \nL 309.096576 162.791599 \nL 309.401244 209.845905 \nL 309.705912 155.415972 \nL 310.01058 188.216443 \nL 310.315249 184.635089 \nL 310.619917 195.021142 \nL 310.924585 155.544514 \nL 311.229254 197.825104 \nL 311.533922 182.876 \nL 311.83859 175.419601 \nL 312.143259 189.474575 \nL 312.447927 190.509015 \nL 312.752595 168.920367 \nL 313.361932 197.122495 \nL 313.6666 176.543272 \nL 313.971268 195.210684 \nL 314.275937 163.559202 \nL 314.580605 202.836023 \nL 314.885273 176.771165 \nL 315.189942 189.755973 \nL 315.49461 176.866539 \nL 315.799278 189.795093 \nL 316.103947 171.379391 \nL 316.408615 181.439219 \nL 316.713283 185.450582 \nL 317.017951 182.296869 \nL 317.32262 189.124365 \nL 317.627288 164.081044 \nL 317.931956 204.347798 \nL 318.236625 162.918229 \nL 318.541293 203.1124 \nL 318.845961 191.297013 \nL 319.15063 169.891842 \nL 319.759966 189.415364 \nL 320.064635 178.228371 \nL 320.369303 203.268398 \nL 320.673971 170.134406 \nL 320.978639 183.978178 \nL 321.283308 192.152081 \nL 321.587976 181.977921 \nL 321.892644 185.588516 \nL 322.197313 184.676311 \nL 322.501981 162.745192 \nL 322.806649 203.470481 \nL 323.111318 172.629973 \nL 323.415986 211.135259 \nL 323.720654 168.986215 \nL 324.025322 192.024572 \nL 324.329991 172.786155 \nL 324.634659 169.072767 \nL 324.939327 211.336801 \nL 325.243996 165.248442 \nL 325.548664 203.696883 \nL 325.853332 165.397671 \nL 326.158001 176.915535 \nL 326.462669 211.436763 \nL 326.767337 171.087051 \nL 327.072006 184.711024 \nL 327.376674 205.95514 \nL 327.681342 158.026151 \nL 328.290679 203.866242 \nL 328.595347 165.893339 \nL 328.900015 190.681974 \nL 329.204684 198.216316 \nL 329.509352 171.775745 \nL 329.81402 198.309581 \nL 330.118689 185.065243 \nL 330.423357 179.433364 \nL 330.728025 185.356191 \nL 331.032693 177.224091 \nL 331.337362 193.166572 \nL 331.64203 190.762512 \nL 331.946698 179.694133 \nL 332.251367 183.170514 \nL 332.556035 179.923207 \nL 332.860703 187.700883 \nL 333.165372 191.030478 \nL 333.47004 179.982411 \nL 333.774708 177.769487 \nL 334.079377 198.84553 \nL 334.384045 185.642454 \nL 334.688713 177.877323 \nL 334.993381 206.750691 \nL 335.29805 164.689371 \nL 335.907386 191.279291 \nL 336.212055 193.551043 \nL 336.516723 172.786028 \nL 336.821391 191.409821 \nL 337.12606 193.822522 \nL 337.430728 177.966421 \nL 337.735396 180.858522 \nL 338.040064 199.192 \nL 338.344733 183.693524 \nL 338.649401 194.102748 \nL 338.954069 175.649436 \nL 339.258738 188.937541 \nL 339.868074 178.48425 \nL 340.172743 199.342408 \nL 340.477411 186.451575 \nL 340.782079 178.53661 \nL 341.086748 186.528714 \nL 341.696084 186.598642 \nL 342.000752 199.97994 \nL 342.610089 173.383426 \nL 343.219426 191.698806 \nL 343.524094 194.831651 \nL 343.828762 187.389469 \nL 344.133431 191.758007 \nL 344.438099 174.210968 \nL 344.742767 194.429959 \nL 345.047436 179.62268 \nL 345.352104 194.496262 \nL 345.656772 187.795528 \nL 345.96144 187.218255 \nL 346.266109 187.248122 \nL 346.570777 173.971521 \nL 346.875445 200.682628 \nL 347.180114 166.002131 \nL 347.78945 205.302006 \nL 348.094119 177.723218 \nL 348.398787 200.907159 \nL 348.703455 179.575859 \nL 349.008123 174.257599 \nL 349.312792 208.160324 \nL 349.61746 188.65894 \nL 349.922128 186.826246 \nL 350.226797 180.630862 \nL 350.531465 208.345142 \nL 351.140802 162.18939 \nL 351.44547 187.98884 \nL 351.750138 196.155795 \nL 352.054807 193.354331 \nL 352.359475 166.618432 \nL 352.664143 200.423663 \nL 352.968811 175.973717 \nL 353.27348 196.409393 \nL 353.578148 188.291264 \nL 353.882816 200.539903 \nL 354.187485 193.613181 \nL 354.492153 176.211377 \nL 354.796821 192.464311 \nL 355.10149 196.699824 \nL 355.406158 176.457769 \nL 355.710826 200.67933 \nL 356.015494 176.596002 \nL 356.320163 187.339838 \nL 356.624831 210.388064 \nL 356.929499 167.174223 \nL 357.538836 206.040293 \nL 358.148173 171.854538 \nL 358.452841 206.129805 \nL 358.757509 189.024742 \nL 359.062178 163.816673 \nL 359.366846 197.471615 \nL 359.671514 189.23918 \nL 359.976182 189.226444 \nL 360.280851 206.268983 \nL 360.585519 172.476635 \nL 360.890187 202.803273 \nL 361.499524 177.708041 \nL 361.804192 194.672938 \nL 362.108861 187.84656 \nL 362.413529 186.35451 \nL 362.718197 189.699462 \nL 363.022865 198.071005 \nL 363.327534 173.143741 \nL 363.632202 211.646223 \nL 363.93687 168.112208 \nL 364.241539 211.768184 \nL 364.546207 179.661163 \nL 364.850875 186.94723 \nL 365.155544 203.537304 \nL 365.460212 176.724007 \nL 365.76488 203.622233 \nL 366.069549 176.822551 \nL 366.374217 215.08278 \nL 366.678885 165.564062 \nL 366.983553 195.335121 \nL 367.288222 188.315242 \nL 367.59289 196.11348 \nL 367.897558 182.082063 \nL 368.202227 195.568938 \nL 368.506895 190.615645 \nL 368.811563 201.863881 \nL 369.116232 183.093407 \nL 369.4209 190.807244 \nL 369.725568 182.345474 \nL 369.725568 182.345474 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 224.966416 \nL 50.14375 7.526416 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 224.966416 \nL 384.94375 7.526416 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 224.966416 \nL 384.94375 224.966416 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.526416 \nL 384.94375 7.526416 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p1e57033364\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"50.14375\" y=\"7.526416\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "you [ 0.92959255  1.0589069   1.3455471  -1.456136    0.58067787]\nsay [-1.3041788  -0.6097657  -0.2928373  -0.02553379 -1.2852652 ]\ngoodbye [ 0.9180985   0.9737682   0.50748545 -0.30862084  1.2210654 ]\nand [-1.092893  -1.34753    1.4959707 -1.415607  -1.0460085]\ni [ 0.92532074  0.9860437   0.5039795  -0.29979742  1.2368453 ]\nhello [ 0.9410774   1.0579448   1.339414   -1.4627044   0.56426626]\n. [-1.1432261  1.577998  -1.2666957  1.2822665 -1.1385578]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "from common.trainer import Trainer\n",
    "from common.optimizer import Adam\n",
    "#from simple_cbow import SimpleCBOW\n",
    "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
    "\n",
    "\n",
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 1000\n",
    "\n",
    "text = 'You say goodbye and I say hello.'\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "you [ 0.92959255  1.0589069   1.3455471  -1.456136    0.58067787]\nsay [-1.3041788  -0.6097657  -0.2928373  -0.02553379 -1.2852652 ]\ngoodbye [ 0.9180985   0.9737682   0.50748545 -0.30862084  1.2210654 ]\nand [-1.092893  -1.34753    1.4959707 -1.415607  -1.0460085]\ni [ 0.92532074  0.9860437   0.5039795  -0.29979742  1.2368453 ]\nhello [ 0.9410774   1.0579448   1.339414   -1.4627044   0.56426626]\n. [-1.1432261  1.577998  -1.2666957  1.2822665 -1.1385578]\n"
     ]
    }
   ],
   "source": [
    "word_vecs = model.word_vecs\n",
    "for word_id, word in id_to_word.items():\n",
    "    print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}