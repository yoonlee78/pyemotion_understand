{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0ef4108c76294684c0f1cba43ee1db9aa02b7944d7b563cb87e2dd407cbf1005a",
   "display_name": "Python 3.7.9 64-bit ('bert': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 5장 순환 신경망(RNN) Part 1\n",
    "### Pyemotion Understand 2021 July 1st\n",
    "### YK Lee\n",
    "\n",
    "# 5.2. RNN이란\n",
    "\n",
    "RNN의 Recurrent는 라틴어에서 온 말로, '몇 번이나 반복해서 일어나는 일'을 뜻한다 우리말로는 '재발한다', '주기적으로 일어난다', '순환한다' 등으로 번역된다. \n",
    "\n",
    "그래서 RNN을 직역하면 '순환하는 신경망'이 된다. 이번 절에서는 이 '순환한다'는 말에 의미를 계속 생각해볼 것이다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.2.1. 순환하는 신경망\n",
    "\n",
    "바로 본론으로 들어가서 '순환한다'에는 어떤 의미가 담겨있을까?\n",
    "\n",
    "- 한 지점에서 시작 => 시간을 지나 다시 원래 장소로 돌아옴\n",
    "- 이 과정을 반복하는 것이 순환\n",
    "- 순환하기 위해 '닫힌 경로'가 필요함\n",
    "- 닫힌 경로 혹은 순환하는 경로가 존재해야 데이터가 같은 장소를 반복해 왕래할 수 있다. 그리고 데이터가 순환하면서 정보가 끊임없이 갱신된다\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/Users/yklee/pyemotion_understand\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "source": [
    "RNN을 구체적으로 살펴보겠다. 여기서는 RNN에 이용되는 계층을 'RNN 계층'이라고 부를 것이다.\n",
    "이 계층은 그림 5-6처럼 그릴 수 있다.\n",
    "\n",
    "<img src = \"./images_equations/fig 5-6.png\" width =300>\n",
    "\n",
    "- x_t를 입력받는다 (t는 시각을 뜻한다)\n",
    "- 시계열 데이터 (x_0, x_1, ...x_t, ...)가 RNN계층에 입력됨을 표현한 것이다. 그리고 입력에 대응하여 (h_0, h_1, ... h_t ...)가 출력된다.\n",
    "\n",
    "각 시각에 입력되는 x_t는 벡터라고 가정하자. 문장 (단어 순서)을 다루는 경우를 예로 든다면 각 단어의 분산 표현 (단어 벡터)이 x_t가 되며, 이 분산 표현이 순서대로 하나씩 RNN계층에 입력된다.\n",
    "\n",
    "- 그림 5-6에 나와있듯이 출력이 2개로 분기하고 있다. 분기 = 같은 것이 복제되어 분기함을 의미한다. 그리고 이렇게 분기된 출력 중 하나가 자기 자신에 입력된다 (즉, 순환한다)\n",
    "\n",
    "<img src = \"./images_equations/fig 5-7.png\" width =300>\n",
    "\n",
    "- 지금까지는 계층을 그릴 때 데이터가 왼쪽에서 오른쪽으로 흐르는 형태로 그렸다. 그러나 지금부터는 지면 관계상 아래에서 위로 흐르도록 그릴 것이다 (곧이어 순환 구조를 펼쳐볼 텐데, 이때 계층을 양옆으로 펼치기 위해서다)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 5.2.2. 순환 구조 펼치기\n",
    "\n",
    "이것으로 준비는 다 되었습니다. 그럼 RNN 계층의 순환 구조에 대해 자세하게 살펴보자. \n",
    " RNN의 순환 구조는 지금까지의 신경망에는 존재하지 않던 구조다. 그러나 이 순환 구조를 펼치면 친숙한 신경망으로 '변신'시킬 수 있다. \n",
    " \n",
    "## <그림 5-8> RNN 계층의 순환 구조 펼치기\n",
    "\n",
    "<img src = \"./images_equations/fig 5-8.png\" width= 800 >\n",
    "\n",
    "[그림 5-8]에서 보듯, RNN 계층의 순환 구조를 펼침으로써 오른쪽으로 성장하는 긴 신경망으로 변신시킬 수 있다. 지금까지 본 피드포워드 신경망과 같은 구조다 (피드포워드는 데이터가 한 방향으로만 흐른다)\n",
    "\n",
    "[그림 5-8]에 등장하는 다수의 RNN계층 모두가 실제로는 '같은 계층'인 것이 지금까지의 신경망과는 다르다.\n",
    "\n",
    "각 시각의 RNN 계층은 그 계층으로의 입력과 1개 전의 RNN 계층으로부터의 출력을 받는다. 그리고 이 두 정보를 바탕으로 현 시각의 출력을 계산한다. 이때 수행하는 계산의 수식은 다음과 같다.\n",
    "\n",
    "<img src = \"./images_equations/e 5-9.png\" width = 300 >\n",
    "\n",
    "[식 5.9]에 쓰인 기호를 살펴보자. RNN에서는 가중치가 2개 있다. 하나는 입력 x를 출력 h로 변환하기 위한 가중치 W_x이고 다른 하나는 1개의 RNN 출력을 다음 시각의 출력으로 변환하기 위한 가중치 W_h이다. 또한 편향 b도 있다. 참고로 h_t-1과 x_t는 행 벡터다.\n",
    "\n",
    "행렬 곱을 계산하고 그 합을 tanh함수 (쌍곡탄젠트 함수)를 이용해 변환한다. 그 결과가 시각 t의 출력 h_t가 된다. 이 h_t는 다른 계층을 향해 위쪽으로 출력되는 동시에 다음 시각의 RNN 계층(자기 자신)을 향해 오른쪽으로도 출력된다.\n",
    "\n",
    "현재의 출력(h_t)은 한 시각 이전 출력(h_t-1)에 기초해 계산됨을 알 수 있다. 다른 관점으로 보면, RNN은 h라는 '상태'를 가지고 있으며, [식 5.9]의 형태로 갱신된다고 해석할 수 있다. 그래서 RNN계층을 '상태를 가지는 계층' 혹은 '메모리(기억력)가 있는 계층' 이라고 한다. \n",
    "\n",
    "많은 문헌에서 펼쳐진 RNN계층을 [그림 5-9]의 왼쪽처럼 그린다.\n",
    "\n",
    "[그림 5-9] 펼쳐진 RNN 계층을 그리는 방식 비교\n",
    "\n",
    "<img src = \"./images_equations/fig 5-9.png\" width=500>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 5.2.3. BPTT\n",
    "\n",
    "RNN계층은 가로로 펼친 신경망으로 간주할 수 있다. 보통의 신경망과 같은 순서로 진행할 수 있다. (그림 5-10처럼)\n",
    "\n",
    "<img src = \"./images_equations/fig 5-10.png\" width =500 >\n",
    "\n",
    "**BPTT** Backpropagation Through Time <시간 방향으로 펼친 신경망의 오차역전파법>\n",
    "\n",
    "긴 시계열 데이터를 학습할 때 문제가 발생. 시계열 데이터의 시간 크기가 커지는 것에 비례하여 BPTT가 소비하는 컴퓨팅 자원도 증가. \n",
    "\n",
    "또한, 시간 크기가 커지면 역전파 시의 기울기가 불안정해진다.\n",
    "\n",
    "## 5.2.4. Truncated BPTT\n",
    "\n",
    "**Truncated BPTT**\n",
    "\n",
    "큰 시계열 데이터를 취급할 때는 흔히 신경망 연결을 적당한 길이로 \"끊는다\".\n",
    "\n",
    "시간축 방향으로 너무 길어진 신경망을 적당한 지점에서 잘라내어 작은 신경망 여러 개로 만든다는 아이디어. \n",
    "\n",
    "그리고 이 잘라낸 작은 신경망에서 오차역전파법을 수행한다 = Truncated BPTT\n",
    "\n",
    "- 신경망의 연결을 끊지만, 제대로 구현하려면 '역전파'의 연결만 끊어야한다. 순전파의 연결은 반드시 그대로 유지해야함. 즉, 순전파의 흐름은 끊어지지 않고 전파된다. 한편, 역전파의 연결은 적당한 길이로 잘라내, 그 잘라낸 신경망 단위로 학습을 수행한다.\n",
    "\n",
    "- 시계열 길이가 너무 길면 계산량과 메모리 사용량 등이 문제가 된다. 계층이 길어짐에 따라 신경망을 하나 통과할 때마다 기울기 값이 조금씩 작아져, 이전 시각t 까지 역전파 되기 전에 0이 되어 소멸할 수도 있다. 이러한 이유로 그림 5-11처럼 적당한 길이로 끊을 생각을 한 것이다.\n",
    "\n",
    "## 그림 5-11 역전파의 연결을 적당한 지점에서 끊는다. 역전파가 연결되는 일련의 RNN계층을 '블록'이라 하고, 배경을 회색으로 칠해 다른 블록과 구분함\n",
    "\n",
    "<img src = \"./images_equations/fig 5-11.png\" width =500>\n",
    "\n",
    "[그림 5-11]에서는 RNN 계층을 길이 10개 단위로 학습할 수 있도록 역전파의 연결을 끊었다. 이처럼 역전파의 연결을 자르면 그보다 미래의 데이터에 대해서는 생각할 필요가 없어진다. 따라서 각각의 블록 단위로, 미래의 블록과는 독립적으로 오차역전파법을 완결시킬 수 있다.\n",
    "\n",
    "여기서 반드시 기억할 점은 역전파의 연결은 끊어지지만, 순전파의 연결은 끊어지지 않는다는 점이다. 그러므로 RNN을 학습시킬 때는 순전파가 연결된다는 점을 고려해야한다. \n",
    "= 데이터를 '순서대로sequential'입력해야한다는 뜻이다. 데이터를 순서대로 입력한다는 것의 구체적인 의미는 다음과 같다.\n",
    "\n",
    "1) Truncated BPTT방식으로 RNN을 학습시키기. \n",
    "\n",
    "첫번째 블록 입력 데이터 (x_0, ... x_9)를 RNN계층에 제공하는 것이다. 그러면 [그림 5-12]와 같은 일이 일어난다.\n",
    "\n",
    "(순전파=>역전파 순으로 수행. 원하는 기울기를 얻는다. 이어서 다음 블록의 입력 데이터 (x10~x19)를 입력해 오차역전파법을 수행)\n",
    "\n",
    "## 그림 5-12 첫 번째 블록의 순전파와 역전파: 이보다 앞선 시각으로부터의 기울기는 끊겼기 때문에 이 블록 내에서만 오차역전파법이 완결된다. \n",
    "\n",
    "<img src = \"./images_equations/fig 5-12.png\" width =500>\n",
    "\n",
    "\n",
    "## 그림 5-13 두 번째 블록의 순전파와 역전파 \n",
    "\n",
    "<img src = \"./images_equations/fig 5-13.png\" width =500>\n",
    "\n",
    "여기서 중요한 점은 이번 순전파 계산에는 앞 블록의 마지막 은닉 상태인 h9가 필요하다는 것이다. 이것으로 순전파는 계속 연결 가능하다.\n",
    "\n",
    "## 그림 5-14 Truncated BPTT의 데이터 처리 순서 \n",
    "\n",
    "<img src = \"./images_equations/fig 5-14.png\" width =500>\n",
    "\n",
    "## 5.2.5. Truncated BPTT의 미니배치 학습\n",
    "\n",
    "## 그림 5-15 미니배치 학습 시 데이터를 제공하는 시작 위치를 각 미니배치 (각 샘플)로 옮긴다.  \n",
    "\n",
    "<img src = \"./images_equations/fig 5-15.png\" width =500>\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- Truncated BPTT의 원리는 단순하지만 '데이터 제공방법' 면에서는 몇 가지를 주의해야한다.\n",
    "1) 데이터를 순서대로 제공하기\n",
    "2) 미니배치별로 데이터를 제공하는 시작 위치를 옮기기"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}